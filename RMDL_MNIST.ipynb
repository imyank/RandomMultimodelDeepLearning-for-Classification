{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RMDL_MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5aJTtzXJmy0",
        "outputId": "0edd6cd1-3cca-456b-f9ac-6c7d78379854"
      },
      "source": [
        "# DOWNLOAD GLOVE\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "import os, sys, tarfile\n",
        "import numpy as np\n",
        "import zipfile\n",
        "\n",
        "if sys.version_info >= (3, 0, 0):\n",
        "    import urllib.request as urllib  # ugly but works\n",
        "else:\n",
        "    import urllib\n",
        "\n",
        "print(sys.version_info)\n",
        "\n",
        "# image shape\n",
        "\n",
        "\n",
        "# path to the directory with the data\n",
        "DATA_DIR = '.\\Glove'\n",
        "\n",
        "# url of the binary data\n",
        "\n",
        "\n",
        "\n",
        "# path to the binary train file with image data\n",
        "\n",
        "\n",
        "def download_and_extract(data='Wikipedia'):\n",
        "    \"\"\"\n",
        "    Download and extract the GloVe\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "\n",
        "    if data=='Wikipedia':\n",
        "        DATA_URL = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
        "    elif data=='Common_Crawl_840B':\n",
        "        DATA_URL = 'http://nlp.stanford.edu/data/wordvecs/glove.840B.300d.zip'\n",
        "    elif data=='Common_Crawl_42B':\n",
        "        DATA_URL = 'http://nlp.stanford.edu/data/wordvecs/glove.42B.300d.zip'\n",
        "    elif data=='Twitter':\n",
        "        DATA_URL = 'http://nlp.stanford.edu/data/wordvecs/glove.twitter.27B.zip'\n",
        "    else:\n",
        "        print(\"prameter should be Twitter, Common_Crawl_42B, Common_Crawl_840B, or Wikipedia\")\n",
        "        exit(0)\n",
        "\n",
        "\n",
        "    dest_directory = DATA_DIR\n",
        "    if not os.path.exists(dest_directory):\n",
        "        os.makedirs(dest_directory)\n",
        "    filename = DATA_URL.split('/')[-1]\n",
        "    filepath = os.path.join(dest_directory, filename)\n",
        "    print(filepath)\n",
        "\n",
        "    path = os.path.abspath(dest_directory)\n",
        "    if not os.path.exists(filepath):\n",
        "        def _progress(count, block_size, total_size):\n",
        "            sys.stdout.write('\\rDownloading %s %.2f%%' % (filename,\n",
        "                                                          float(count * block_size) / float(total_size) * 100.0))\n",
        "            sys.stdout.flush()\n",
        "\n",
        "        filepath, _ = urllib.urlretrieve(DATA_URL, filepath)#, reporthook=_progress)\n",
        "\n",
        "\n",
        "        zip_ref = zipfile.ZipFile(filepath, 'r')\n",
        "        zip_ref.extractall(DATA_DIR)\n",
        "        zip_ref.close()\n",
        "    return path\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sys.version_info(major=3, minor=6, micro=9, releaselevel='final', serial=0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxdRuEg5M2CF",
        "outputId": "ba0f12c0-a0f2-4730-858f-a1c5608f12bb"
      },
      "source": [
        "# DOWNLOAD WOS\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "import os, sys, tarfile\n",
        "import numpy as np\n",
        "\n",
        "if sys.version_info >= (3, 0, 0):\n",
        "    import urllib.request as urllib  # ugly but works\n",
        "else:\n",
        "    import urllib\n",
        "\n",
        "print(sys.version_info)\n",
        "\n",
        "# image shape\n",
        "\n",
        "\n",
        "# path to the directory with the data\n",
        "DATA_DIR = '.\\data_WOS'\n",
        "\n",
        "# url of the binary data\n",
        "DATA_URL = 'http://kowsari.net/WebOfScience.tar.gz'\n",
        "\n",
        "\n",
        "# path to the binary train file with image data\n",
        "\n",
        "\n",
        "def download_and_extract():\n",
        "    \"\"\"\n",
        "    Download and extract the WOS datasets\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    dest_directory = DATA_DIR\n",
        "    if not os.path.exists(dest_directory):\n",
        "        os.makedirs(dest_directory)\n",
        "    filename = DATA_URL.split('/')[-1]\n",
        "    filepath = os.path.join(dest_directory, filename)\n",
        "\n",
        "\n",
        "    path = os.path.abspath(dest_directory)\n",
        "    if not os.path.exists(filepath):\n",
        "        def _progress(count, block_size, total_size):\n",
        "            sys.stdout.write('\\rDownloading %s %.2f%%' % (filename,\n",
        "                                                          float(count * block_size) / float(total_size) * 100.0))\n",
        "            sys.stdout.flush()\n",
        "\n",
        "        filepath, _ = urllib.urlretrieve(DATA_URL, filepath, reporthook=_progress)\n",
        "\n",
        "        print('Downloaded', filename)\n",
        "\n",
        "        tarfile.open(filepath, 'r').extractall(dest_directory)\n",
        "    return path\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sys.version_info(major=3, minor=6, micro=9, releaselevel='final', serial=0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iu_wC9mLM8J4"
      },
      "source": [
        "# MODEL\n",
        "\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "from keras.models import Sequential\n",
        "import numpy as np\n",
        "from keras.constraints import maxnorm\n",
        "from keras.layers import Dense, Flatten\n",
        "from keras.layers import Conv1D,MaxPooling2D, \\\n",
        "    MaxPooling1D, Embedding, Dropout,\\\n",
        "    GRU,TimeDistributed,Conv2D,\\\n",
        "    Activation,LSTM,Input\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.layers.core import Lambda\n",
        "from keras.layers.merge import Concatenate\n",
        "import tensorflow as tf\n",
        "from keras import optimizers\n",
        "import random\n",
        "\n",
        "def optimizors(random_optimizor):\n",
        "    if random_optimizor:\n",
        "        i = random.randint(1,3)\n",
        "        if i==0:\n",
        "            opt = optimizers.SGD()\n",
        "        elif i==1:\n",
        "            opt= optimizers.RMSprop()\n",
        "        elif i==2:\n",
        "            opt= optimizers.Adagrad()\n",
        "        elif i==3:\n",
        "            opt = optimizers.Adam()\n",
        "        elif i==4:\n",
        "            opt =optimizers.Nadam()\n",
        "        print(opt)\n",
        "    else:\n",
        "        opt= optimizers.Adam()\n",
        "    return opt\n",
        "\n",
        "\n",
        "\n",
        "def slice_batch(x, n_gpus, part):\n",
        "    \"\"\"\n",
        "    Divide the input batch into [n_gpus] slices, and obtain slice number [part].\n",
        "    i.e. if len(x)=10, then slice_batch(x, 2, 1) will return x[5:].\n",
        "    \"\"\"\n",
        "\n",
        "    sh = K.shape(x)\n",
        "    L = sh[0] // n_gpus\n",
        "    if part == n_gpus - 1:\n",
        "        return x[part*L:]\n",
        "    return x[part*L:(part+1)*L]\n",
        "\n",
        "def to_multi_gpu(model, n_gpus=2):\n",
        "    \"\"\"\n",
        "    Given a keras [model], return an equivalent model which parallelizes\n",
        "    the computation over [n_gpus] GPUs.\n",
        "\n",
        "    Each GPU gets a slice of the input batch, applies the model on that slice\n",
        "    and later the outputs of the models are concatenated to a single tensor,\n",
        "    hence the user sees a model that behaves the same as the original.\n",
        "    \"\"\"\n",
        "\n",
        "    with tf.device('/cpu:0'):\n",
        "        x = Input(model.input_shape[1:], name=\"input1\")\n",
        "\n",
        "    towers = []\n",
        "    for g in range(n_gpus):\n",
        "        with tf.device('/gpu:' + str(g)):\n",
        "            slice_g = Lambda(slice_batch,\n",
        "                             lambda shape: shape,\n",
        "                             arguments={'n_gpus':n_gpus, 'part':g})(x)\n",
        "            towers.append(model(slice_g))\n",
        "\n",
        "    with tf.device('/cpu:0'):\n",
        "        merged = Concatenate(axis=0)(towers)\n",
        "\n",
        "    return Model(inputs=[x], outputs=[merged])\n",
        "\n",
        "\n",
        "def Build_Model_DNN_Image(shape, number_of_classes, sparse_categorical, min_hidden_layer_dnn,max_hidden_layer_dnn,\n",
        "                          min_nodes_dnn, max_nodes_dnn, random_optimizor, dropout):\n",
        "    '''\n",
        "    buildModel_DNN_image(shape, nClasses,sparse_categorical)\n",
        "    Build Deep neural networks Model for image classification\n",
        "    Shape is input feature space\n",
        "    nClasses is number of classes\n",
        "    '''\n",
        "\n",
        "    model = Sequential()\n",
        "    values = list(range(min_nodes_dnn,max_nodes_dnn))\n",
        "    Numberof_NOde = random.choice(values)\n",
        "    Lvalues = list(range(min_hidden_layer_dnn,max_hidden_layer_dnn))\n",
        "    nLayers =random.choice(Lvalues)\n",
        "    print(shape)\n",
        "    model.add(Flatten(input_shape=shape))\n",
        "    model.add(Dense(Numberof_NOde,activation='relu'))\n",
        "    model.add(Dropout(dropout))\n",
        "    for i in range(0,nLayers-1):\n",
        "        Numberof_NOde = random.choice(values)\n",
        "        model.add(Dense(Numberof_NOde,activation='relu'))\n",
        "        model.add(Dropout(dropout))\n",
        "    model.add(Dense(number_of_classes, activation='softmax'))\n",
        "    model_tmp = model\n",
        "    if sparse_categorical:\n",
        "        model.compile(loss='sparse_categorical_crossentropy',\n",
        "                      optimizer=optimizors(random_optimizor),\n",
        "                      metrics=['accuracy'])\n",
        "    else:\n",
        "        model.compile(loss='categorical_crossentropy',\n",
        "                      optimizer=optimizors(random_optimizor),\n",
        "                      metrics=['accuracy'])\n",
        "    return model,model_tmp\n",
        "\n",
        "\n",
        "def Build_Model_DNN_Text(shape, nClasses, sparse_categorical,\n",
        "                         min_hidden_layer_dnn, max_hidden_layer_dnn, min_nodes_dnn,\n",
        "                         max_nodes_dnn, random_optimizor, dropout):\n",
        "    \"\"\"\n",
        "    buildModel_DNN_Tex(shape, nClasses,sparse_categorical)\n",
        "    Build Deep neural networks Model for text classification\n",
        "    Shape is input feature space\n",
        "    nClasses is number of classes\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "    layer = list(range(min_hidden_layer_dnn,max_hidden_layer_dnn))\n",
        "    node = list(range(min_nodes_dnn, max_nodes_dnn))\n",
        "\n",
        "\n",
        "    Numberof_NOde =  random.choice(node)\n",
        "    nLayers = random.choice(layer)\n",
        "\n",
        "    Numberof_NOde_old = Numberof_NOde\n",
        "    model.add(Dense(Numberof_NOde,input_dim=shape,activation='relu'))\n",
        "    model.add(Dropout(dropout))\n",
        "    for i in range(0,nLayers):\n",
        "        Numberof_NOde = random.choice(node)\n",
        "        model.add(Dense(Numberof_NOde,input_dim=Numberof_NOde_old,activation='relu'))\n",
        "        model.add(Dropout(dropout))\n",
        "        Numberof_NOde_old = Numberof_NOde\n",
        "    model.add(Dense(nClasses, activation='softmax'))\n",
        "    model_tem = model\n",
        "    if sparse_categorical:\n",
        "        model.compile(loss='sparse_categorical_crossentropy',\n",
        "                      optimizer=optimizors(random_optimizor),\n",
        "                      metrics=['accuracy'])\n",
        "    else:\n",
        "        model.compile(loss='categorical_crossentropy',\n",
        "                      optimizer=optimizors(random_optimizor),\n",
        "                      metrics=['accuracy'])\n",
        "    return model,model_tem\n",
        "\n",
        "\n",
        "def Build_Model_CNN_Image(shape, nclasses, sparse_categorical,\n",
        "                          min_hidden_layer_cnn, max_hidden_layer_cnn, min_nodes_cnn,\n",
        "                          max_nodes_cnn, random_optimizor, dropout):\n",
        "    \"\"\"\"\"\n",
        "    def Image_model_CNN(num_classes,shape):\n",
        "    num_classes is number of classes,\n",
        "    shape is (w,h,p)\n",
        "    \"\"\"\"\"\n",
        "\n",
        "    model = Sequential()\n",
        "    values = list(range(min_nodes_cnn,max_nodes_cnn))\n",
        "    Layers = list(range(min_hidden_layer_cnn, max_hidden_layer_cnn))\n",
        "    Layer = random.choice(Layers)\n",
        "    Filter = random.choice(values)\n",
        "    model.add(Conv2D(Filter, (3, 3), padding='same', input_shape=shape))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(Filter, (3, 3)))\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    for i in range(0,Layer):\n",
        "        Filter = random.choice(values)\n",
        "        model.add(Conv2D(Filter, (3, 3),padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(nclasses,activation='softmax',kernel_constraint=maxnorm(3)))\n",
        "    model_tmp = model\n",
        "    if sparse_categorical:\n",
        "        model.compile(loss='sparse_categorical_crossentropy',\n",
        "                      optimizer=optimizors(random_optimizor),\n",
        "                      metrics=['accuracy'])\n",
        "    else:\n",
        "        model.compile(loss='categorical_crossentropy',\n",
        "                      optimizer=optimizors(random_optimizor),\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "    return model,model_tmp\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def Build_Model_RNN_Image(shape,\n",
        "                          nclasses,\n",
        "                          sparse_categorical,\n",
        "                          min_nodes_rnn,\n",
        "                          max_nodes_rnn,\n",
        "                          random_optimizor,\n",
        "                          dropout):\n",
        "    \"\"\"\n",
        "        def Image_model_RNN(num_classes,shape):\n",
        "        num_classes is number of classes,\n",
        "        shape is (w,h,p)\n",
        "    \"\"\"\n",
        "    values = list(range(min_nodes_rnn,max_nodes_rnn))\n",
        "    node =  random.choice(values)\n",
        "\n",
        "    x = Input(shape=shape)\n",
        "\n",
        "    # Encodes a row of pixels using TimeDistributed Wrapper.\n",
        "    encoded_rows = TimeDistributed(LSTM(node,recurrent_dropout=dropout))(x)\n",
        "    node = random.choice(values)\n",
        "    # Encodes columns of encoded rows.\n",
        "    encoded_columns = LSTM(node,recurrent_dropout=dropout)(encoded_rows)\n",
        "\n",
        "    # Final predictions and model.\n",
        "    #prediction = Dense(256, activation='relu')(encoded_columns)\n",
        "    prediction = Dense(nclasses, activation='softmax')(encoded_columns)\n",
        "    model = Model(x, prediction)\n",
        "    model_tmp = model\n",
        "    if sparse_categorical:\n",
        "        model.compile(loss='sparse_categorical_crossentropy',\n",
        "                  optimizer=optimizors(random_optimizor),\n",
        "                  metrics=['accuracy'])\n",
        "    else:\n",
        "        model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=optimizors(random_optimizor),\n",
        "                  metrics=['accuracy'])\n",
        "    return model,model_tmp\n",
        "\n",
        "\n",
        "def Build_Model_RNN_Text(word_index, embeddings_index, nclasses,  MAX_SEQUENCE_LENGTH, EMBEDDING_DIM, sparse_categorical,\n",
        "                         min_hidden_layer_rnn, max_hidden_layer_rnn, min_nodes_rnn, max_nodes_rnn, random_optimizor, dropout):\n",
        "    \"\"\"\n",
        "    def buildModel_RNN(word_index, embeddings_index, nClasses, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM, sparse_categorical):\n",
        "    word_index in word index ,\n",
        "    embeddings_index is embeddings index, look at data_helper.py\n",
        "    nClasses is number of classes,\n",
        "    MAX_SEQUENCE_LENGTH is maximum lenght of text sequences\n",
        "    \"\"\"\n",
        "\n",
        "    model = Sequential()\n",
        "    values = list(range(min_nodes_rnn,max_nodes_rnn))\n",
        "    values_layer = list(range(min_hidden_layer_rnn,max_hidden_layer_rnn))\n",
        "\n",
        "    layer = random.choice(values_layer)\n",
        "    print(layer)\n",
        "    embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
        "    for word, i in word_index.items():\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # words not found in embedding index will be all-zeros.\n",
        "            if len(embedding_matrix[i]) != len(embedding_vector):\n",
        "                print(\"could not broadcast input array from shape\", str(len(embedding_matrix[i])),\n",
        "                      \"into shape\", str(len(embedding_vector)), \" Please make sure your\"\n",
        "                                                                \" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,\")\n",
        "                exit(1)\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "    model.add(Embedding(len(word_index) + 1,\n",
        "                                EMBEDDING_DIM,\n",
        "                                weights=[embedding_matrix],\n",
        "                                input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                trainable=True))\n",
        "\n",
        "    gru_node = random.choice(values)\n",
        "    print(gru_node)\n",
        "    for i in range(0,layer):\n",
        "        model.add(GRU(gru_node,return_sequences=True, recurrent_dropout=0.2))\n",
        "        model.add(Dropout(dropout))\n",
        "    model.add(GRU(gru_node, recurrent_dropout=0.2))\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dense(nclasses, activation='softmax'))\n",
        "\n",
        "    model_tmp = model\n",
        "    #model = to_multi_gpu(model, 3)\n",
        "\n",
        "\n",
        "    if sparse_categorical:\n",
        "        model.compile(loss='sparse_categorical_crossentropy',\n",
        "                      optimizer=optimizors(random_optimizor),\n",
        "                      metrics=['accuracy'])\n",
        "    else:\n",
        "        model.compile(loss='categorical_crossentropy',\n",
        "                      optimizer=optimizors(random_optimizor),\n",
        "                      metrics=['accuracy'])\n",
        "    return model,model_tmp\n",
        "\n",
        "\n",
        "def Build_Model_CNN_Text(word_index, embeddings_index, nclasses, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM, sparse_categorical,\n",
        "                       min_hidden_layer_cnn, max_hidden_layer_cnn, min_nodes_cnn, max_nodes_cnn, random_optimizor,\n",
        "                       dropout, simple_model=False):\n",
        "\n",
        "    \"\"\"\n",
        "        def buildModel_CNN(word_index,embeddings_index,nClasses,MAX_SEQUENCE_LENGTH,EMBEDDING_DIM,Complexity=0):\n",
        "        word_index in word index ,\n",
        "        embeddings_index is embeddings index, look at data_helper.py\n",
        "        nClasses is number of classes,\n",
        "        MAX_SEQUENCE_LENGTH is maximum lenght of text sequences,\n",
        "        EMBEDDING_DIM is an int value for dimention of word embedding look at data_helper.py\n",
        "        Complexity we have two different CNN model as follows\n",
        "        F=0 is simple CNN with [1 5] hidden layer\n",
        "        Complexity=2 is more complex model of CNN with filter_length of range [1 10]\n",
        "    \"\"\"\n",
        "\n",
        "    model = Sequential()\n",
        "    if simple_model:\n",
        "        embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
        "        for word, i in word_index.items():\n",
        "            embedding_vector = embeddings_index.get(word)\n",
        "            if embedding_vector is not None:\n",
        "                if len(embedding_matrix[i]) !=len(embedding_vector):\n",
        "                    print(\"could not broadcast input array from shape\",str(len(embedding_matrix[i])),\n",
        "                                     \"into shape\",str(len(embedding_vector)),\" Please make sure your\"\n",
        "                                     \" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,\")\n",
        "                    exit(1)\n",
        "                # words not found in embedding index will be all-zeros.\n",
        "                embedding_matrix[i] = embedding_vector\n",
        "        model.add(Embedding(len(word_index) + 1,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            trainable=True))\n",
        "        values = list(range(min_nodes_cnn,max_nodes_cnn))\n",
        "        Layer = list(range(min_hidden_layer_cnn,max_hidden_layer_cnn))\n",
        "        Layer = random.choice(Layer)\n",
        "        for i in range(0,Layer):\n",
        "            Filter = random.choice(values)\n",
        "            model.add(Conv1D(Filter, 5, activation='relu'))\n",
        "            model.add(Dropout(dropout))\n",
        "            model.add(MaxPooling1D(5))\n",
        "\n",
        "        model.add(Flatten())\n",
        "        Filter = random.choice(values)\n",
        "        model.add(Dense(Filter, activation='relu'))\n",
        "        model.add(Dropout(dropout))\n",
        "        Filter = random.choice(values)\n",
        "        model.add(Dense(Filter, activation='relu'))\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(nclasses, activation='softmax'))\n",
        "        model_tmp = model\n",
        "        #model = Model(sequence_input, preds)\n",
        "        if sparse_categorical:\n",
        "            model.compile(loss='sparse_categorical_crossentropy',\n",
        "                          optimizer=optimizors(random_optimizor),\n",
        "                          metrics=['accuracy'])\n",
        "        else:\n",
        "            model.compile(loss='categorical_crossentropy',\n",
        "                          optimizer=optimizors(random_optimizor),\n",
        "                          metrics=['accuracy'])\n",
        "    else:\n",
        "        embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
        "        for word, i in word_index.items():\n",
        "            embedding_vector = embeddings_index.get(word)\n",
        "            if embedding_vector is not None:\n",
        "                # words not found in embedding index will be all-zeros.\n",
        "                if len(embedding_matrix[i]) !=len(embedding_vector):\n",
        "                    print(\"could not broadcast input array from shape\",str(len(embedding_matrix[i])),\n",
        "                                     \"into shape\",str(len(embedding_vector)),\" Please make sure your\"\n",
        "                                     \" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,\")\n",
        "                    exit(1)\n",
        "\n",
        "                embedding_matrix[i] = embedding_vector\n",
        "\n",
        "        embedding_layer = Embedding(len(word_index) + 1,\n",
        "                                    EMBEDDING_DIM,\n",
        "                                    weights=[embedding_matrix],\n",
        "                                    input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                    trainable=True)\n",
        "\n",
        "        # applying a more complex convolutional approach\n",
        "        convs = []\n",
        "        values_layer = list(range(min_hidden_layer_cnn,max_hidden_layer_cnn))\n",
        "        filter_sizes = []\n",
        "        layer = random.choice(values_layer)\n",
        "        print(\"Filter  \",layer)\n",
        "        for fl in range(0,layer):\n",
        "            filter_sizes.append((fl+2))\n",
        "\n",
        "        values_node = list(range(min_nodes_cnn,max_nodes_cnn))\n",
        "        node = random.choice(values_node)\n",
        "        print(\"Node  \", node)\n",
        "        sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "        embedded_sequences = embedding_layer(sequence_input)\n",
        "\n",
        "        for fsz in filter_sizes:\n",
        "            l_conv = Conv1D(node, kernel_size=fsz, activation='relu')(embedded_sequences)\n",
        "            l_pool = MaxPooling1D(5)(l_conv)\n",
        "            #l_pool = Dropout(0.25)(l_pool)\n",
        "            convs.append(l_pool)\n",
        "\n",
        "        l_merge = Concatenate(axis=1)(convs)\n",
        "        l_cov1 = Conv1D(node, 5, activation='relu')(l_merge)\n",
        "        l_cov1 = Dropout(dropout)(l_cov1)\n",
        "        l_pool1 = MaxPooling1D(5)(l_cov1)\n",
        "        l_cov2 = Conv1D(node, 5, activation='relu')(l_pool1)\n",
        "        l_cov2 = Dropout(dropout)(l_cov2)\n",
        "        l_pool2 = MaxPooling1D(30)(l_cov2)\n",
        "        l_flat = Flatten()(l_pool2)\n",
        "        l_dense = Dense(1024, activation='relu')(l_flat)\n",
        "        l_dense = Dropout(dropout)(l_dense)\n",
        "        l_dense = Dense(512, activation='relu')(l_dense)\n",
        "        l_dense = Dropout(dropout)(l_dense)\n",
        "        preds = Dense(nclasses, activation='softmax')(l_dense)\n",
        "        model = Model(sequence_input, preds)\n",
        "        model_tmp = model\n",
        "        if sparse_categorical:\n",
        "            model.compile(loss='sparse_categorical_crossentropy',\n",
        "                          optimizer=optimizors(random_optimizor),\n",
        "                          metrics=['accuracy'])\n",
        "        else:\n",
        "            model.compile(loss='categorical_crossentropy',\n",
        "                          optimizer=optimizors(random_optimizor),\n",
        "                          metrics=['accuracy'])\n",
        "\n",
        "\n",
        "    return model,model_tmp\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dfDAkVCNGUV"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def setup():\n",
        "    np.set_printoptions(threshold=np.inf)\n",
        "    np.random.seed(7)\n",
        "    if not os.path.exists(\".\\weights\"):\n",
        "        os.makedirs(\".\\weights\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNL3Zt_1NIwM"
      },
      "source": [
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from pylab import *\n",
        "import itertools\n",
        "\n",
        "def RMDL_epoch(history_):\n",
        "    Number_of_models = len(history_)\n",
        "    caption=[]\n",
        "    for i in range(0,len(history_)):\n",
        "        caption.append('RDL '+str(i+1))\n",
        "    plt.legend(caption, loc='upper right')\n",
        "    for i in range(0, Number_of_models):\n",
        "        plt.plot(history_[i].history['accuracy'])\n",
        "        plt.title('model train accuracy')\n",
        "        plt.ylabel('accuracy')\n",
        "        plt.xlabel('epoch')\n",
        "    plt.legend(caption, loc='upper right')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    for i in range(0, Number_of_models):\n",
        "        plt.plot(history_[i].history['val_accuracy'])\n",
        "        plt.title('model test accuracy')\n",
        "        plt.ylabel('accuracy')\n",
        "        plt.xlabel('epoch')\n",
        "\n",
        "    plt.show()\n",
        "    plt.legend(caption, loc='upper right')\n",
        "    for i in range(0, Number_of_models):\n",
        "        # summarize history for loss\n",
        "        plt.plot(history_[i].history['loss'])\n",
        "\n",
        "        plt.title('model train loss ')\n",
        "        plt.ylabel('loss')\n",
        "        plt.xlabel('epoch')\n",
        "\n",
        "    plt.legend(caption, loc='upper right')\n",
        "    plt.show()\n",
        "    plt.legend(\n",
        "        caption, loc='upper right')\n",
        "    for i in range(0, Number_of_models):\n",
        "        # summarize history for loss\n",
        "        plt.plot(history_[i].history['val_loss'])\n",
        "\n",
        "        plt.title('model loss test')\n",
        "        plt.ylabel('loss')\n",
        "        plt.xlabel('epoch')\n",
        "    plt.legend(caption, loc='upper right')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        #print(\"Normalized confusion matrix\")\n",
        "    #else:\n",
        "       # print('Confusion matrix, without normalization')\n",
        "\n",
        "    #print(cm)\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def accuracy(y_test,final_y):\n",
        "    np.set_printoptions(precision=2)\n",
        "    y_test_temp = np.argmax(y_test, axis=1)\n",
        "    F_score = accuracy_score(y_test_temp, final_y)\n",
        "    F1 = precision_recall_fscore_support(y_test_temp, final_y, average='micro')\n",
        "    F2 = precision_recall_fscore_support(y_test_temp, final_y, average='macro')\n",
        "    F3 = precision_recall_fscore_support(y_test_temp, final_y, average='weighted')\n",
        "    print(F_score)\n",
        "    print(F1)\n",
        "    print(F2)\n",
        "    print(F3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AdFP4SgNK4d",
        "outputId": "0c45ccb9-6ecc-467f-b4c8-f71a7a7e1064"
      },
      "source": [
        "!pip install RMDL\n",
        "# FOR IMAGE CLASSIFICATION\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from RMDL import Plot as Plot\n",
        "import gc\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import collections\n",
        "from sklearn.metrics import f1_score\n",
        "from RMDL import BuildModel as BuildModel\n",
        "from RMDL import Global as G\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "np.random.seed(7)\n",
        "\n",
        "\n",
        "def Image_Classification(x_train, y_train, x_test, y_test, shape, batch_size=128,\n",
        "                         sparse_categorical=True, random_deep=[3, 3, 3], epochs=[500, 500, 500], plot=False,\n",
        "                         min_hidden_layer_dnn=1, max_hidden_layer_dnn=8, min_nodes_dnn=128, max_nodes_dnn=1024,\n",
        "                         max_hidden_layer_rnn=5, min_nodes_rnn=32, max_nodes_rnn=128,\n",
        "                         min_hidden_layer_cnn=3, max_hidden_layer_cnn=10, min_nodes_cnn=128, max_nodes_cnn=512,\n",
        "                         random_state=42, random_optimizor=True, dropout=0.05):\n",
        "    \"\"\"\n",
        "    def Image_Classification(x_train, y_train, x_test, y_test, shape, batch_size=128,\n",
        "                             sparse_categorical=True, random_deep=[3, 3, 3], epochs=[500, 500, 500], plot=False,\n",
        "                             min_hidden_layer_dnn=1, max_hidden_layer_dnn=8, min_nodes_dnn=128, max_nodes_dnn=1024,\n",
        "                             min_hidden_layer_rnn=1, max_hidden_layer_rnn=5, min_nodes_rnn=32, max_nodes_rnn=128,\n",
        "                             min_hidden_layer_cnn=3, max_hidden_layer_cnn=10, min_nodes_cnn=128, max_nodes_cnn=512,\n",
        "                             random_state=42, random_optimizor=True, dropout=0.05):\n",
        "\n",
        "            Parameters\n",
        "            ----------\n",
        "                x_train : string\n",
        "                    input X for training\n",
        "                y_train : int\n",
        "                    input Y for training\n",
        "                x_test : string\n",
        "                    input X for testing\n",
        "                x_test : int\n",
        "                    input Y for testing\n",
        "                shape : np.shape\n",
        "                    shape of image. The most common situation would be a 2D input with shape (batch_size, input_dim).\n",
        "                batch_size : Integer, , optional\n",
        "                    Number of samples per gradient update. If unspecified, it will default to 128\n",
        "                MAX_NB_WORDS: int, optional\n",
        "                    Maximum number of unique words in datasets, it will default to 75000.\n",
        "                GloVe_dir: String, optional\n",
        "                    Address of GloVe or any pre-trained directory, it will default to null which glove.6B.zip will be download.\n",
        "                GloVe_dir: String, optional\n",
        "                    Which version of GloVe or pre-trained word emending will be used, it will default to glove.6B.50d.txt.\n",
        "                    NOTE: if you use other version of GloVe EMBEDDING_DIM must be same dimensions.\n",
        "                sparse_categorical: bool.\n",
        "                    When target's dataset is (n,1) should be True, it will default to True.\n",
        "                random_deep: array of int [3], optional\n",
        "                    Number of ensembled model used in RMDL random_deep[0] is number of DNN, random_deep[1] is number of RNN, random_deep[0] is number of CNN, it will default to [3, 3, 3].\n",
        "                epochs: array of int [3], optional\n",
        "                    Number of epochs in each ensembled model used in RMDL epochs[0] is number of epochs used in DNN, epochs[1] is number of epochs used in RNN, epochs[0] is number of epochs used in CNN, it will default to [500, 500, 500].\n",
        "                plot: bool, optional\n",
        "                    True: shows confusion matrix and accuracy and loss\n",
        "                min_hidden_layer_dnn: Integer, optional\n",
        "                    Lower Bounds of hidden layers of DNN used in RMDL, it will default to 1.\n",
        "                max_hidden_layer_dnn: Integer, optional\n",
        "                    Upper bounds of hidden layers of DNN used in RMDL, it will default to 8.\n",
        "                min_nodes_dnn: Integer, optional\n",
        "                    Lower bounds of nodes in each layer of DNN used in RMDL, it will default to 128.\n",
        "                max_nodes_dnn: Integer, optional\n",
        "                    Upper bounds of nodes in each layer of DNN used in RMDL, it will default to 1024.\n",
        "                min_hidden_layer_rnn: Integer, optional\n",
        "                    Lower Bounds of hidden layers of RNN used in RMDL, it will default to 1.\n",
        "                min_hidden_layer_rnn: Integer, optional\n",
        "                    Upper Bounds of hidden layers of RNN used in RMDL, it will default to 5.\n",
        "                min_nodes_rnn: Integer, optional\n",
        "                    Lower bounds of nodes (LSTM or GRU) in each layer of RNN used in RMDL, it will default to 32.\n",
        "                max_nodes_rnn: Integer, optional\n",
        "                    Upper bounds of nodes (LSTM or GRU) in each layer of RNN used in RMDL, it will default to 128.\n",
        "                min_hidden_layer_cnn: Integer, optional\n",
        "                    Lower Bounds of hidden layers of CNN used in RMDL, it will default to 3.\n",
        "                max_hidden_layer_cnn: Integer, optional\n",
        "                    Upper Bounds of hidden layers of CNN used in RMDL, it will default to 10.\n",
        "                min_nodes_cnn: Integer, optional\n",
        "                    Lower bounds of nodes (2D convolution layer) in each layer of CNN used in RMDL, it will default to 128.\n",
        "                min_nodes_cnn: Integer, optional\n",
        "                    Upper bounds of nodes (2D convolution layer) in each layer of CNN used in RMDL, it will default to 512.\n",
        "                random_state : Integer, optional\n",
        "                    RandomState instance or None, optional (default=None)\n",
        "                    If Integer, random_state is the seed used by the random number generator;\n",
        "                random_optimizor : bool, optional\n",
        "                    If False, all models use adam optimizer. If True, all models use random optimizers. it will default to True\n",
        "                dropout: Float, optional\n",
        "                    between 0 and 1. Fraction of the units to drop for the linear transformation of the inputs.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "    if len(x_train) != len(y_train):\n",
        "        raise ValueError('shape of x_train and y_train must be equal'\n",
        "                         'The x_train has ' + str(len(x_train)) +\n",
        "                         'The x_train has' +\n",
        "                         str(len(y_train)))\n",
        "\n",
        "    if len(x_test) != len(y_test):\n",
        "        raise ValueError('shape of x_test and y_test must be equal '\n",
        "                         'The x_train has ' + str(len(x_test)) +\n",
        "                         'The y_test has ' +\n",
        "                         str(len(y_test)))\n",
        "\n",
        "    np.random.seed(random_state)\n",
        "    G.setup()\n",
        "    y_proba = []\n",
        "\n",
        "    score = []\n",
        "    history_ = []\n",
        "    if sparse_categorical:\n",
        "        number_of_classes = np.max(y_train)+1\n",
        "    else:\n",
        "        number_of_classes = np.shape(y_train)[0]\n",
        "\n",
        "    i =0\n",
        "    while i < random_deep[0]:\n",
        "        try:\n",
        "            print(\"DNN \", i, \"\\n\")\n",
        "            model_DNN, model_tmp = BuildModel.Build_Model_DNN_Image(shape,\n",
        "                                                                    number_of_classes,\n",
        "                                                                    sparse_categorical,\n",
        "                                                                    min_hidden_layer_dnn,\n",
        "                                                                    max_hidden_layer_dnn,\n",
        "                                                                    min_nodes_dnn,\n",
        "                                                                    max_nodes_dnn,\n",
        "                                                                    random_optimizor,\n",
        "                                                                    dropout)\n",
        "\n",
        "\n",
        "            filepath = \"weights\\weights_DNN_\" + str(i) + \".hdf5\"\n",
        "            checkpoint = ModelCheckpoint(filepath,\n",
        "                                         monitor='val_accuracy',\n",
        "                                         verbose=1,\n",
        "                                         save_best_only=True,\n",
        "                                         mode='max')\n",
        "            callbacks_list = [checkpoint]\n",
        "\n",
        "            history = model_DNN.fit(x_train, y_train,\n",
        "                                    validation_data=(x_test, y_test),\n",
        "                                    epochs=epochs[0],\n",
        "                                    batch_size=batch_size,\n",
        "                                    callbacks=callbacks_list,\n",
        "                                    verbose=2)\n",
        "            history_.append(history)\n",
        "            model_tmp.load_weights(filepath)\n",
        "\n",
        "            if sparse_categorical == 0:\n",
        "                model_tmp.compile(loss='sparse_categorical_crossentropy',\n",
        "                                  optimizer='adam',\n",
        "                                  metrics=['accuracy'])\n",
        "            else:\n",
        "                model_tmp.compile(loss='categorical_crossentropy',\n",
        "                                  optimizer='adam',\n",
        "                                  metrics=['accuracy'])\n",
        "\n",
        "            y_pr = model_tmp.predict_classes(x_test, batch_size=batch_size)\n",
        "            y_proba.append(np.array(y_pr))\n",
        "            score.append(accuracy_score(y_test, y_pr))\n",
        "            i = i + 1\n",
        "            del model_tmp\n",
        "            del model_DNN\n",
        "            gc.collect()\n",
        "        except:\n",
        "            print(\"Error in model\", i, \"try to re-generate an other model\")\n",
        "            if max_hidden_layer_dnn > 3:\n",
        "                max_hidden_layer_dnn -= 1\n",
        "            if max_nodes_dnn > 256:\n",
        "                max_nodes_dnn -= 8\n",
        "\n",
        "\n",
        "    i =0\n",
        "    while i < random_deep[1]:\n",
        "        try:\n",
        "            print(\"RNN \", i, \"\\n\")\n",
        "            model_RNN, model_tmp = BuildModel.Build_Model_RNN_Image(shape,\n",
        "                                                                    number_of_classes,\n",
        "                                                                    sparse_categorical,\n",
        "                                                                    min_nodes_rnn,\n",
        "                                                                    max_nodes_rnn,\n",
        "                                                                    random_optimizor,\n",
        "                                                                    dropout)\n",
        "\n",
        "            filepath = \"weights\\weights_RNN_\" + str(i) + \".hdf5\"\n",
        "            checkpoint = ModelCheckpoint(filepath,\n",
        "                                         monitor='val_accuracy',\n",
        "                                         verbose=1,\n",
        "                                         save_best_only=True,\n",
        "                                         mode='max')\n",
        "            callbacks_list = [checkpoint]\n",
        "\n",
        "            history = model_RNN.fit(x_train, y_train,\n",
        "                                    validation_data=(x_test, y_test),\n",
        "                                    epochs=epochs[1],\n",
        "                                    batch_size=batch_size,\n",
        "                                    verbose=2,\n",
        "                                    callbacks=callbacks_list)\n",
        "\n",
        "            model_tmp.load_weights(filepath)\n",
        "            model_tmp.compile(loss='sparse_categorical_crossentropy',\n",
        "                              optimizer='rmsprop',\n",
        "                              metrics=['accuracy'])\n",
        "            history_.append(history)\n",
        "\n",
        "            y_pr = model_tmp.predict(x_test, batch_size=batch_size)\n",
        "            y_pr = np.argmax(y_pr, axis=1)\n",
        "            y_proba.append(np.array(y_pr))\n",
        "            score.append(accuracy_score(y_test, y_pr))\n",
        "            i = i+1\n",
        "            del model_tmp\n",
        "            del model_RNN\n",
        "            gc.collect()\n",
        "        except:\n",
        "            print(\"Error in model\", i, \" try to re-generate another model\")\n",
        "            if max_hidden_layer_rnn > 3:\n",
        "                max_hidden_layer_rnn -= 1\n",
        "            if max_nodes_rnn > 64:\n",
        "                max_nodes_rnn -= 2\n",
        "\n",
        "    # reshape to be [samples][pixels][width][height]\n",
        "    i=0\n",
        "    while i < random_deep[2]:\n",
        "        try:\n",
        "            print(\"CNN \", i, \"\\n\")\n",
        "            model_CNN, model_tmp = BuildModel.Build_Model_CNN_Image(shape,\n",
        "                                                                    number_of_classes,\n",
        "                                                                    sparse_categorical,\n",
        "                                                                    min_hidden_layer_cnn,\n",
        "                                                                    max_hidden_layer_cnn,\n",
        "                                                                    min_nodes_cnn,\n",
        "                                                                    max_nodes_cnn,\n",
        "                                                                    random_optimizor,\n",
        "                                                                    dropout)\n",
        "\n",
        "            filepath = \"weights\\weights_CNN_\" + str(i) + \".hdf5\"\n",
        "            checkpoint = ModelCheckpoint(filepath, \n",
        "                                         monitor='val_accuracy',\n",
        "                                         verbose=1, \n",
        "                                         save_best_only=True,\n",
        "                                         mode='max')\n",
        "            callbacks_list = [checkpoint]\n",
        "\n",
        "            history = model_CNN.fit(x_train, y_train,\n",
        "                                    validation_data=(x_test, y_test),\n",
        "                                    epochs=epochs[2],\n",
        "                                    batch_size=batch_size,\n",
        "                                    callbacks=callbacks_list,\n",
        "                                    verbose=2)\n",
        "            history_.append(history)\n",
        "            model_tmp.load_weights(filepath)\n",
        "            model_tmp.compile(loss='sparse_categorical_crossentropy',\n",
        "                              optimizer='adam',\n",
        "                              metrics=['accuracy'])\n",
        "\n",
        "            y_pr = model_tmp.predict_classes(x_test, batch_size=batch_size)\n",
        "            y_proba.append(np.array(y_pr))\n",
        "            score.append(accuracy_score(y_test, y_pr))\n",
        "            i = i+1\n",
        "            del model_tmp\n",
        "            del model_CNN\n",
        "            gc.collect()\n",
        "        except:\n",
        "            print(\"Error in model\", i, \" try to re-generate another model\")\n",
        "            if max_hidden_layer_cnn > 5:\n",
        "                max_hidden_layer_cnn -= 1\n",
        "            if max_nodes_cnn > 128:\n",
        "                max_nodes_cnn -= 2\n",
        "                min_nodes_cnn -= 1\n",
        "\n",
        "\n",
        "\n",
        "    y_proba = np.array(y_proba).transpose()\n",
        "    print(y_proba.shape)\n",
        "    final_y = []\n",
        "    for i in range(0, y_proba.shape[0]):\n",
        "        a = np.array(y_proba[i, :])\n",
        "        a = collections.Counter(a).most_common()[0][0]\n",
        "        final_y.append(a)\n",
        "    F_score = accuracy_score(y_test, final_y)\n",
        "    F1 = f1_score(y_test, final_y, average='micro')\n",
        "    F2 = f1_score(y_test, final_y, average='macro')\n",
        "    F3 = f1_score(y_test, final_y, average='weighted')\n",
        "    cnf_matrix = confusion_matrix(y_test, final_y)\n",
        "    # Compute confusion matrix\n",
        "    np.set_printoptions(precision=2)\n",
        "    if plot:\n",
        "        # Plot non-normalized confusion matrix\n",
        "        classes = list(range(0,np.max(y_test)+1))\n",
        "        Plot.plot_confusion_matrix(cnf_matrix, classes=classes,\n",
        "                         title='Confusion matrix, without normalization')\n",
        "        Plot.plot_confusion_matrix(cnf_matrix, classes=classes,normalize=True,\n",
        "                              title='Confusion matrix, without normalization')\n",
        "\n",
        "    if plot:\n",
        "        Plot.RMDL_epoch(history_)\n",
        "\n",
        "    print(y_proba.shape)\n",
        "    print(\"Accuracy of\",len(score),\"models:\",score)\n",
        "    print(\"Accuracy:\",np.amax(score))\n",
        "    print(\"Accuracy F Score:\",F_score)\n",
        "    print(\"F1_Micro:\",F1)\n",
        "    print(\"F1_Macro:\",F2)\n",
        "    print(\"F1_weighted:\",F3)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting RMDL\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/1c/7911d9b8ea3a95983d19720560963b3b809af7308a46a111756606ed928f/RMDL-1.0.8-py2.py3-none-any.whl (44kB)\n",
            "\r\u001b[K     |███████▍                        | 10kB 20.2MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 20kB 26.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 30kB 26.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 40kB 20.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 5.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=2.1.2 in /usr/local/lib/python3.6/dist-packages (from RMDL) (3.2.2)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.6/dist-packages (from RMDL) (1.1.5)\n",
            "Requirement already satisfied: keras>=2.0.9 in /usr/local/lib/python3.6/dist-packages (from RMDL) (2.4.3)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (from RMDL) (2.4.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from RMDL) (1.4.1)\n",
            "Requirement already satisfied: nltk>=3.2.4 in /usr/local/lib/python3.6/dist-packages (from RMDL) (3.2.5)\n",
            "Requirement already satisfied: scikit-learn>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from RMDL) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.6/dist-packages (from RMDL) (1.19.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.2->RMDL) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.2->RMDL) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.2->RMDL) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.2->RMDL) (2.4.7)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.22.0->RMDL) (2018.9)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.9->RMDL) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.9->RMDL) (2.10.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->RMDL) (3.3.0)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->RMDL) (1.32.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow->RMDL) (3.7.4.3)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->RMDL) (1.1.2)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow->RMDL) (2.4.1)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow->RMDL) (0.3.3)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->RMDL) (1.15.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tensorflow->RMDL) (0.36.2)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow->RMDL) (0.10.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->RMDL) (1.12.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->RMDL) (2.4.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->RMDL) (1.12)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->RMDL) (0.2.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->RMDL) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->RMDL) (3.12.4)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow->RMDL) (1.6.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.19.0->RMDL) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow->RMDL) (3.3.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow->RMDL) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow->RMDL) (0.4.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow->RMDL) (53.0.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow->RMDL) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow->RMDL) (1.25.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow->RMDL) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow->RMDL) (3.4.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow->RMDL) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow->RMDL) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow->RMDL) (4.7)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow->RMDL) (4.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow->RMDL) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow->RMDL) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow->RMDL) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow->RMDL) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow->RMDL) (3.4.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow->RMDL) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow->RMDL) (0.4.8)\n",
            "Installing collected packages: RMDL\n",
            "Successfully installed RMDL-1.0.8\n",
            "sys.version_info(major=3, minor=6, micro=9, releaselevel='final', serial=0)\n",
            "sys.version_info(major=3, minor=6, micro=9, releaselevel='final', serial=0)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "377ni0W2NPjp"
      },
      "source": [
        "#TEXT CLASSIFICATION\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "import gc\n",
        "import os\n",
        "import numpy as np\n",
        "import collections\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from RMDL import BuildModel as BuildModel\n",
        "from RMDL.Download import Download_Glove as GloVe\n",
        "from RMDL import text_feature_extraction as txt\n",
        "from RMDL import Global as G\n",
        "from RMDL import Plot as Plot\n",
        "\n",
        "\n",
        "def Text_Classification(x_train, y_train, x_test,  y_test, batch_size=128,\n",
        "                        EMBEDDING_DIM=50,MAX_SEQUENCE_LENGTH = 500, MAX_NB_WORDS = 75000,\n",
        "                        GloVe_dir=\"\", GloVe_file = \"glove.6B.50d.txt\",\n",
        "                        sparse_categorical=True, random_deep=[3, 3, 3], epochs=[500, 500, 500],  plot=False,\n",
        "                        min_hidden_layer_dnn=1, max_hidden_layer_dnn=8, min_nodes_dnn=128, max_nodes_dnn=1024,\n",
        "                        min_hidden_layer_rnn=1, max_hidden_layer_rnn=5, min_nodes_rnn=32,  max_nodes_rnn=128,\n",
        "                        min_hidden_layer_cnn=3, max_hidden_layer_cnn=10, min_nodes_cnn=128, max_nodes_cnn=512,\n",
        "                        random_state=42, random_optimizor=True, dropout=0.5,no_of_classes=0):\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    Text_Classification(x_train, y_train, x_test,  y_test, batch_size=128,\n",
        "                        EMBEDDING_DIM=50,MAX_SEQUENCE_LENGTH = 500, MAX_NB_WORDS = 75000,\n",
        "                        GloVe_dir=\"\", GloVe_file = \"glove.6B.50d.txt\",\n",
        "                        sparse_categorical=True, random_deep=[3, 3, 3], epochs=[500, 500, 500],  plot=False,\n",
        "                        min_hidden_layer_dnn=1, max_hidden_layer_dnn=8, min_nodes_dnn=128, max_nodes_dnn=1024,\n",
        "                        min_hidden_layer_rnn=1, max_hidden_layer_rnn=5, min_nodes_rnn=32,  max_nodes_rnn=128,\n",
        "                        min_hidden_layer_cnn=3, max_hidden_layer_cnn=10, min_nodes_cnn=128, max_nodes_cnn=512,\n",
        "                        random_state=42, random_optimizor=True, dropout=0.5):\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "            batch_size : Integer, , optional\n",
        "                Number of samples per gradient update. If unspecified, it will default to 128\n",
        "            MAX_NB_WORDS: int, optional\n",
        "                Maximum number of unique words in datasets, it will default to 75000.\n",
        "            GloVe_dir: String, optional\n",
        "                Address of GloVe or any pre-trained directory, it will default to null which glove.6B.zip will be download.\n",
        "            GloVe_dir: String, optional\n",
        "                Which version of GloVe or pre-trained word emending will be used, it will default to glove.6B.50d.txt.\n",
        "                NOTE: if you use other version of GloVe EMBEDDING_DIM must be same dimensions.\n",
        "            sparse_categorical: bool.\n",
        "                When target's dataset is (n,1) should be True, it will default to True.\n",
        "            random_deep: array of int [3], optional\n",
        "                Number of ensembled model used in RMDL random_deep[0] is number of DNN, random_deep[1] is number of RNN, random_deep[0] is number of CNN, it will default to [3, 3, 3].\n",
        "            epochs: array of int [3], optional\n",
        "                Number of epochs in each ensembled model used in RMDL epochs[0] is number of epochs used in DNN, epochs[1] is number of epochs used in RNN, epochs[0] is number of epochs used in CNN, it will default to [500, 500, 500].\n",
        "            plot: bool, optional\n",
        "                True: shows confusion matrix and accuracy and loss\n",
        "            min_hidden_layer_dnn: Integer, optional\n",
        "                Lower Bounds of hidden layers of DNN used in RMDL, it will default to 1.\n",
        "            max_hidden_layer_dnn: Integer, optional\n",
        "                Upper bounds of hidden layers of DNN used in RMDL, it will default to 8.\n",
        "            min_nodes_dnn: Integer, optional\n",
        "                Lower bounds of nodes in each layer of DNN used in RMDL, it will default to 128.\n",
        "            max_nodes_dnn: Integer, optional\n",
        "                Upper bounds of nodes in each layer of DNN used in RMDL, it will default to 1024.\n",
        "            min_hidden_layer_rnn: Integer, optional\n",
        "                Lower Bounds of hidden layers of RNN used in RMDL, it will default to 1.\n",
        "            min_hidden_layer_rnn: Integer, optional\n",
        "                Upper Bounds of hidden layers of RNN used in RMDL, it will default to 5.\n",
        "            min_nodes_rnn: Integer, optional\n",
        "                Lower bounds of nodes (LSTM or GRU) in each layer of RNN used in RMDL, it will default to 32.\n",
        "            max_nodes_rnn: Integer, optional\n",
        "                Upper bounds of nodes (LSTM or GRU) in each layer of RNN used in RMDL, it will default to 128.\n",
        "            min_hidden_layer_cnn: Integer, optional\n",
        "                Lower Bounds of hidden layers of CNN used in RMDL, it will default to 3.\n",
        "            max_hidden_layer_cnn: Integer, optional\n",
        "                Upper Bounds of hidden layers of CNN used in RMDL, it will default to 10.\n",
        "            min_nodes_cnn: Integer, optional\n",
        "                Lower bounds of nodes (2D convolution layer) in each layer of CNN used in RMDL, it will default to 128.\n",
        "            min_nodes_cnn: Integer, optional\n",
        "                Upper bounds of nodes (2D convolution layer) in each layer of CNN used in RMDL, it will default to 512.\n",
        "            random_state : Integer, optional\n",
        "                RandomState instance or None, optional (default=None)\n",
        "                If Integer, random_state is the seed used by the random number generator;\n",
        "            random_optimizor : bool, optional\n",
        "                If False, all models use adam optimizer. If True, all models use random optimizers. it will default to True\n",
        "            dropout: Float, optional\n",
        "                between 0 and 1. Fraction of the units to drop for the linear transformation of the inputs.\n",
        "\n",
        "    \"\"\"\n",
        "    np.random.seed(random_state)\n",
        "\n",
        "\n",
        "    glove_directory = GloVe_dir\n",
        "    GloVe_file = GloVe_file\n",
        "\n",
        "    print(\"Done1\")\n",
        "\n",
        "    GloVe_needed = random_deep[1] != 0 or random_deep[2] != 0\n",
        "    \n",
        "    # example_input  = [0,1,3]\n",
        "    # example_output :\n",
        "    # \n",
        "    # [[1 0 0 0]\n",
        "    #  [0 1 0 0]\n",
        "    #  [0 0 0 1]]\n",
        "    \n",
        "    def one_hot_encoder(value, label_data_):\n",
        "\n",
        "        label_data_[value] = 1\n",
        "\n",
        "        return label_data_\n",
        "\n",
        "    def _one_hot_values(labels_data):\n",
        "        encoded = [0] * len(labels_data)\n",
        "\n",
        "        for index_no, value in enumerate(labels_data):\n",
        "            max_value = [0] * (np.max(labels_data) + 1)\n",
        "\n",
        "            encoded[index_no] = one_hot_encoder(value, max_value)\n",
        "\n",
        "        return np.array(encoded)\n",
        "\n",
        "    if not isinstance(y_train[0], list) and not isinstance(y_train[0], np.ndarray) and not sparse_categorical:\n",
        "        #checking if labels are one hot or not otherwise dense_layer will give shape error \n",
        "        \n",
        "        print(\"converted_into_one_hot\")\n",
        "        y_train = _one_hot_values(y_train)\n",
        "        y_test = _one_hot_values(y_test)\n",
        "            \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    if GloVe_needed:\n",
        "        if glove_directory == \"\":\n",
        "            GloVe_directory = GloVe.download_and_extract()\n",
        "            GloVe_DIR = os.path.join(GloVe_directory, GloVe_file)\n",
        "        else:\n",
        "            GloVe_DIR = os.path.join(glove_directory, GloVe_file)\n",
        "\n",
        "        if not os.path.isfile(GloVe_DIR):\n",
        "            print(\"Could not find %s Set GloVe Directory in Global.py \", GloVe)\n",
        "            exit()\n",
        "\n",
        "    G.setup()\n",
        "    if random_deep[0] != 0:\n",
        "        x_train_tfidf, x_test_tfidf = txt.loadData(x_train, x_test,MAX_NB_WORDS=MAX_NB_WORDS)\n",
        "    if random_deep[1] != 0 or random_deep[2] != 0 :\n",
        "        print(GloVe_DIR)\n",
        "        x_train_embedded, x_test_embedded, word_index, embeddings_index = txt.loadData_Tokenizer(x_train, x_test,GloVe_DIR,MAX_NB_WORDS,MAX_SEQUENCE_LENGTH,EMBEDDING_DIM)\n",
        "\n",
        "    del x_train\n",
        "    del x_test\n",
        "    gc.collect()\n",
        "\n",
        "    y_pr = []\n",
        "    History = []\n",
        "    score = []\n",
        "\n",
        "    if no_of_classes==0:\n",
        "        #checking no_of_classes\n",
        "        #np.max(data)+1 will not work for one_hot encoding labels\n",
        "        if sparse_categorical:\n",
        "            number_of_classes = np.max(y_train) + 1\n",
        "        else:\n",
        "            number_of_classes = len(y_train[0])\n",
        "    else:\n",
        "        number_of_classes = no_of_classes\n",
        "    print(number_of_classes)\n",
        "\n",
        "\n",
        "    i = 0\n",
        "    while i < random_deep[0]:\n",
        "        # model_DNN.append(Sequential())\n",
        "        try:\n",
        "            print(\"DNN \" + str(i))\n",
        "            filepath = \"weights\\weights_DNN_\" + str(i) + \".hdf5\"\n",
        "            checkpoint = ModelCheckpoint(filepath,\n",
        "                                         monitor='val_accuracy',\n",
        "                                         verbose=1,\n",
        "                                         save_best_only=True,\n",
        "                                         mode='max')\n",
        "            callbacks_list = [checkpoint]\n",
        "\n",
        "            model_DNN, model_tmp = BuildModel.Build_Model_DNN_Text(x_train_tfidf.shape[1],\n",
        "                                                                   number_of_classes,\n",
        "                                                                   sparse_categorical,\n",
        "                                                                   min_hidden_layer_dnn,\n",
        "                                                                   max_hidden_layer_dnn,\n",
        "                                                                   min_nodes_dnn,\n",
        "                                                                   max_nodes_dnn,\n",
        "                                                                   random_optimizor,\n",
        "                                                                   dropout)\n",
        "            model_history = model_DNN.fit(x_train_tfidf, y_train,\n",
        "                              validation_data=(x_test_tfidf, y_test),\n",
        "                              epochs=epochs[0],\n",
        "                              batch_size=batch_size,\n",
        "                              callbacks=callbacks_list,\n",
        "                              verbose=2)\n",
        "            History.append(model_history)\n",
        "\n",
        "            model_tmp.load_weights(filepath)\n",
        "            if sparse_categorical:\n",
        "                model_tmp.compile(loss='sparse_categorical_crossentropy',\n",
        "                                  optimizer='adam',\n",
        "                                  metrics=['accuracy'])\n",
        "\n",
        "                y_pr_ = model_tmp.predict_classes(x_test_tfidf,\n",
        "                                                  batch_size=batch_size)\n",
        "                y_pr.append(np.array(y_pr_))\n",
        "                score.append(accuracy_score(y_test, y_pr_))\n",
        "            else:\n",
        "                model_tmp.compile(loss='categorical_crossentropy',\n",
        "                                  optimizer='adam',\n",
        "                                  metrics=['accuracy'])\n",
        "\n",
        "                y_pr_ = model_tmp.predict(x_test_tfidf,\n",
        "                                          batch_size=batch_size)\n",
        "\n",
        "                y_pr_ = np.argmax(y_pr_, axis=1)\n",
        "                y_pr.append(np.array(y_pr_))\n",
        "                y_test_temp = np.argmax(y_test, axis=1)\n",
        "                score.append(accuracy_score(y_test_temp, y_pr_))\n",
        "            # print(y_proba)\n",
        "            i += 1\n",
        "            del model_tmp\n",
        "            del model_DNN\n",
        "\n",
        "        except Exception as e:\n",
        "\n",
        "            print(\"Check the Error \\n {} \".format(e))\n",
        "\n",
        "            print(\"Error in model\", i, \"try to re-generate another model\")\n",
        "            if max_hidden_layer_dnn > 3:\n",
        "                max_hidden_layer_dnn -= 1\n",
        "            if max_nodes_dnn > 256:\n",
        "                max_nodes_dnn -= 8\n",
        "\n",
        "    try:\n",
        "        del x_train_tfidf\n",
        "        del x_test_tfidf\n",
        "        gc.collect()\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    i=0\n",
        "    while i < random_deep[1]:\n",
        "        try:\n",
        "            print(\"RNN \" + str(i))\n",
        "            filepath = \"weights\\weights_RNN_\" + str(i) + \".hdf5\"\n",
        "            checkpoint = ModelCheckpoint(filepath,\n",
        "                                         monitor='val_accuracy',\n",
        "                                         verbose=1,\n",
        "                                         save_best_only=True,\n",
        "                                         mode='max')\n",
        "            callbacks_list = [checkpoint]\n",
        "\n",
        "            model_RNN, model_tmp = BuildModel.Build_Model_RNN_Text(word_index,\n",
        "                                                                   embeddings_index,\n",
        "                                                                   number_of_classes,\n",
        "                                                                   MAX_SEQUENCE_LENGTH,\n",
        "                                                                   EMBEDDING_DIM,\n",
        "                                                                   sparse_categorical,\n",
        "                                                                   min_hidden_layer_rnn,\n",
        "                                                                   max_hidden_layer_rnn,\n",
        "                                                                   min_nodes_rnn,\n",
        "                                                                   max_nodes_rnn,\n",
        "                                                                   random_optimizor,\n",
        "                                                                   dropout)\n",
        "\n",
        "            model_history = model_RNN.fit(x_train_embedded, y_train,\n",
        "                              validation_data=(x_test_embedded, y_test),\n",
        "                              epochs=epochs[1],\n",
        "                              batch_size=batch_size,\n",
        "                              callbacks=callbacks_list,\n",
        "                              verbose=2)\n",
        "            History.append(model_history)\n",
        "\n",
        "            if sparse_categorical:\n",
        "                model_tmp.load_weights(filepath)\n",
        "                model_tmp.compile(loss='sparse_categorical_crossentropy',\n",
        "                                  optimizer='rmsprop',\n",
        "                                  metrics=['accuracy'])\n",
        "\n",
        "                y_pr_ = model_tmp.predict_classes(x_test_embedded, batch_size=batch_size)\n",
        "                y_pr.append(np.array(y_pr_))\n",
        "                score.append(accuracy_score(y_test, y_pr_))\n",
        "            else:\n",
        "                model_tmp.load_weights(filepath)\n",
        "                model_tmp.compile(loss='categorical_crossentropy',\n",
        "                                  optimizer='rmsprop',\n",
        "                                  metrics=['accuracy'])\n",
        "                y_pr_ = model_tmp.predict(x_test_embedded, batch_size=batch_size)\n",
        "                y_pr_ = np.argmax(y_pr_, axis=1)\n",
        "                y_pr.append(np.array(y_pr_))\n",
        "                y_test_temp = np.argmax(y_test, axis=1)\n",
        "                score.append(accuracy_score(y_test_temp, y_pr_))\n",
        "            i += 1\n",
        "            del model_tmp\n",
        "            del model_RNN\n",
        "            gc.collect()\n",
        "        except:\n",
        "            print(\"Error in model\", i, \"try to re-generate another model\")\n",
        "            if max_hidden_layer_rnn > 3:\n",
        "                max_hidden_layer_rnn -= 1\n",
        "            if max_nodes_rnn > 64:\n",
        "                max_nodes_rnn -= 2\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "    i = 0\n",
        "    while i < random_deep[2]:\n",
        "        try:\n",
        "            print(\"CNN \" + str(i))\n",
        "\n",
        "            model_CNN, model_tmp = BuildModel.Build_Model_CNN_Text(word_index,\n",
        "                                                                   embeddings_index,\n",
        "                                                                   number_of_classes,\n",
        "                                                                   MAX_SEQUENCE_LENGTH,\n",
        "                                                                   EMBEDDING_DIM,\n",
        "                                                                   sparse_categorical,\n",
        "                                                                   min_hidden_layer_cnn,\n",
        "                                                                   max_hidden_layer_cnn,\n",
        "                                                                   min_nodes_cnn,\n",
        "                                                                   max_nodes_cnn,\n",
        "                                                                   random_optimizor,\n",
        "                                                                   dropout)\n",
        "\n",
        "\n",
        "\n",
        "            filepath = \"weights\\weights_CNN_\" + str(i) + \".hdf5\"\n",
        "            checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True,\n",
        "                                         mode='max')\n",
        "            callbacks_list = [checkpoint]\n",
        "\n",
        "            model_history = model_CNN.fit(x_train_embedded, y_train,\n",
        "                                          validation_data=(x_test_embedded, y_test),\n",
        "                                          epochs=epochs[2],\n",
        "                                          batch_size=batch_size,\n",
        "                                          callbacks=callbacks_list,\n",
        "                                          verbose=2)\n",
        "            History.append(model_history)\n",
        "\n",
        "            model_tmp.load_weights(filepath)\n",
        "            if sparse_categorical:\n",
        "                model_tmp.compile(loss='sparse_categorical_crossentropy',\n",
        "                                  optimizer='rmsprop',\n",
        "                                  metrics=['accuracy'])\n",
        "            else:\n",
        "                model_tmp.compile(loss='categorical_crossentropy',\n",
        "                                  optimizer='rmsprop',\n",
        "                                  metrics=['accuracy'])\n",
        "\n",
        "            y_pr_ = model_tmp.predict(x_test_embedded, batch_size=batch_size)\n",
        "            y_pr_ = np.argmax(y_pr_, axis=1)\n",
        "            y_pr.append(np.array(y_pr_))\n",
        "\n",
        "            if sparse_categorical:\n",
        "                score.append(accuracy_score(y_test, y_pr_))\n",
        "            else:\n",
        "                y_test_temp = np.argmax(y_test, axis=1)\n",
        "                score.append(accuracy_score(y_test_temp, y_pr_))\n",
        "            i += 1\n",
        "\n",
        "            del model_tmp\n",
        "            del model_CNN\n",
        "            gc.collect()\n",
        "        except:\n",
        "            print(\"Error in model\", i, \"try to re-generate an other model\")\n",
        "            if max_hidden_layer_cnn > 5:\n",
        "                max_hidden_layer_cnn -= 1\n",
        "            if max_nodes_cnn > 128:\n",
        "                max_nodes_cnn -= 2\n",
        "                min_nodes_cnn -= 1\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "    y_proba = np.array(y_pr).transpose()\n",
        "\n",
        "    final_y = []\n",
        "\n",
        "    for i in range(0, y_proba.shape[0]):\n",
        "        a = np.array(y_proba[i, :])\n",
        "        a = collections.Counter(a).most_common()[0][0]\n",
        "        final_y.append(a)\n",
        "    if sparse_categorical:\n",
        "        F_score = accuracy_score(y_test, final_y)\n",
        "        F1 = precision_recall_fscore_support(y_test, final_y, average='micro')\n",
        "        F2 = precision_recall_fscore_support(y_test, final_y, average='macro')\n",
        "        F3 = precision_recall_fscore_support(y_test, final_y, average='weighted')\n",
        "        cnf_matrix = confusion_matrix(y_test, final_y)\n",
        "        # Compute confusion matrix\n",
        "        # Plot non-normalized confusion matrix\n",
        "\n",
        "        if plot:\n",
        "            classes = list(range(0, np.max(y_test)+1))\n",
        "            Plot.plot_confusion_matrix(cnf_matrix, classes=classes,\n",
        "                                       title='Confusion matrix, without normalization')\n",
        "\n",
        "            # Plot normalized confusion matrix\n",
        "\n",
        "            Plot.plot_confusion_matrix(cnf_matrix, classes=classes, normalize=True,\n",
        "                                       title='Normalized confusion matrix')\n",
        "    else:\n",
        "        y_test_temp = np.argmax(y_test, axis=1)\n",
        "        F_score = accuracy_score(y_test_temp, final_y)\n",
        "        F1 = precision_recall_fscore_support(y_test_temp, final_y, average='micro')\n",
        "        F2 = precision_recall_fscore_support(y_test_temp, final_y, average='macro')\n",
        "        F3 = precision_recall_fscore_support(y_test_temp, final_y, average='weighted')\n",
        "    if plot:\n",
        "        Plot.RMDL_epoch(History)\n",
        "    print(y_proba.shape)\n",
        "    print(\"Accuracy of\",len(score),\"models:\",score)\n",
        "    print(\"Accuracy:\",np.amax(score))\n",
        "    # print(\"Accuracy:\",F_score)\n",
        "    print(\"F1_Micro:\",F1)\n",
        "    print(\"F1_Macro:\",F2)\n",
        "    print(\"F1_weighted:\",F3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t43THPpnNd-b",
        "outputId": "074b691f-fa3f-4d03-a410-755f75cc7c31"
      },
      "source": [
        "# TEXT FEATURE EXTRACTION\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "nltk.download(\"stopwords\")\n",
        "cachedStopWords = stopwords.words(\"english\")\n",
        "\n",
        "def transliterate(line):\n",
        "    cedilla2latin = [[u'Á', u'A'], [u'á', u'a'], [u'Č', u'C'], [u'č', u'c'], [u'Š', u'S'], [u'š', u's']]\n",
        "    tr = dict([(a[0], a[1]) for (a) in cedilla2latin])\n",
        "    new_line = \"\"\n",
        "    for letter in line:\n",
        "        if letter in tr:\n",
        "            new_line += tr[letter]\n",
        "        else:\n",
        "            new_line += letter\n",
        "    return new_line\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lerJNpx0pfy5"
      },
      "source": [
        "## **Testing on ensemble(RMDL) model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTnyErA5NiX2",
        "outputId": "22a34ae2-4efd-4efe-b2df-36d7c95a148a"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train_D = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32')\n",
        "X_test_D = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32')\n",
        "X_train = X_train_D / 255.0\n",
        "X_test = X_test_D / 255.0\n",
        "number_of_classes = np.unique(y_train).shape[0]\n",
        "shape = (28, 28, 1)\n",
        "\n",
        "batch_size = 128\n",
        "sparse_categorical = 0\n",
        "n_epochs = [10, 10, 10]  ## DNN-LSTM_GRU-CNN\n",
        "Random_Deep = [2, 2, 2]  ## DNN-LSTM_GRU-CNN\n",
        "\n",
        "Image_Classification(X_train, y_train, X_test, y_test,shape,\n",
        "                     batch_size=batch_size,\n",
        "                     sparse_categorical=True,\n",
        "                     random_deep=Random_Deep,\n",
        "                     epochs=n_epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "DNN  0 \n",
            "\n",
            "(28, 28, 1)\n",
            "<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f30f0088780>\n",
            "Epoch 1/10\n",
            "469/469 - 2s - loss: 1.9783 - accuracy: 0.4798 - val_loss: 1.5678 - val_accuracy: 0.7330\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.73300, saving model to weights\\weights_DNN_0.hdf5\n",
            "Epoch 2/10\n",
            "469/469 - 1s - loss: 1.2574 - accuracy: 0.7574 - val_loss: 0.9357 - val_accuracy: 0.8194\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.73300 to 0.81940, saving model to weights\\weights_DNN_0.hdf5\n",
            "Epoch 3/10\n",
            "469/469 - 1s - loss: 0.8317 - accuracy: 0.8137 - val_loss: 0.6680 - val_accuracy: 0.8486\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.81940 to 0.84860, saving model to weights\\weights_DNN_0.hdf5\n",
            "Epoch 4/10\n",
            "469/469 - 1s - loss: 0.6495 - accuracy: 0.8398 - val_loss: 0.5458 - val_accuracy: 0.8669\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.84860 to 0.86690, saving model to weights\\weights_DNN_0.hdf5\n",
            "Epoch 5/10\n",
            "469/469 - 1s - loss: 0.5568 - accuracy: 0.8554 - val_loss: 0.4773 - val_accuracy: 0.8796\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.86690 to 0.87960, saving model to weights\\weights_DNN_0.hdf5\n",
            "Epoch 6/10\n",
            "469/469 - 1s - loss: 0.5003 - accuracy: 0.8671 - val_loss: 0.4334 - val_accuracy: 0.8870\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.87960 to 0.88700, saving model to weights\\weights_DNN_0.hdf5\n",
            "Epoch 7/10\n",
            "469/469 - 1s - loss: 0.4620 - accuracy: 0.8748 - val_loss: 0.4036 - val_accuracy: 0.8928\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.88700 to 0.89280, saving model to weights\\weights_DNN_0.hdf5\n",
            "Epoch 8/10\n",
            "469/469 - 1s - loss: 0.4348 - accuracy: 0.8801 - val_loss: 0.3807 - val_accuracy: 0.8979\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.89280 to 0.89790, saving model to weights\\weights_DNN_0.hdf5\n",
            "Epoch 9/10\n",
            "469/469 - 1s - loss: 0.4133 - accuracy: 0.8842 - val_loss: 0.3634 - val_accuracy: 0.9013\n",
            "\n",
            "Epoch 00009: val_accuracy improved from 0.89790 to 0.90130, saving model to weights\\weights_DNN_0.hdf5\n",
            "Epoch 10/10\n",
            "469/469 - 1s - loss: 0.3962 - accuracy: 0.8889 - val_loss: 0.3491 - val_accuracy: 0.9041\n",
            "\n",
            "Epoch 00010: val_accuracy improved from 0.90130 to 0.90410, saving model to weights\\weights_DNN_0.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DNN  1 \n",
            "\n",
            "(28, 28, 1)\n",
            "<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f3087d54f60>\n",
            "Epoch 1/10\n",
            "469/469 - 2s - loss: 2.2299 - accuracy: 0.3093 - val_loss: 2.0861 - val_accuracy: 0.6040\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.60400, saving model to weights\\weights_DNN_1.hdf5\n",
            "Epoch 2/10\n",
            "469/469 - 1s - loss: 1.7649 - accuracy: 0.6338 - val_loss: 1.1800 - val_accuracy: 0.7552\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.60400 to 0.75520, saving model to weights\\weights_DNN_1.hdf5\n",
            "Epoch 3/10\n",
            "469/469 - 1s - loss: 0.9146 - accuracy: 0.7668 - val_loss: 0.6203 - val_accuracy: 0.8330\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.75520 to 0.83300, saving model to weights\\weights_DNN_1.hdf5\n",
            "Epoch 4/10\n",
            "469/469 - 1s - loss: 0.6057 - accuracy: 0.8262 - val_loss: 0.4630 - val_accuracy: 0.8712\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.83300 to 0.87120, saving model to weights\\weights_DNN_1.hdf5\n",
            "Epoch 5/10\n",
            "469/469 - 1s - loss: 0.4962 - accuracy: 0.8546 - val_loss: 0.3959 - val_accuracy: 0.8855\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.87120 to 0.88550, saving model to weights\\weights_DNN_1.hdf5\n",
            "Epoch 6/10\n",
            "469/469 - 1s - loss: 0.4364 - accuracy: 0.8713 - val_loss: 0.3570 - val_accuracy: 0.8938\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.88550 to 0.89380, saving model to weights\\weights_DNN_1.hdf5\n",
            "Epoch 7/10\n",
            "469/469 - 1s - loss: 0.4005 - accuracy: 0.8806 - val_loss: 0.3324 - val_accuracy: 0.9031\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.89380 to 0.90310, saving model to weights\\weights_DNN_1.hdf5\n",
            "Epoch 8/10\n",
            "469/469 - 1s - loss: 0.3755 - accuracy: 0.8885 - val_loss: 0.3114 - val_accuracy: 0.9099\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.90310 to 0.90990, saving model to weights\\weights_DNN_1.hdf5\n",
            "Epoch 9/10\n",
            "469/469 - 1s - loss: 0.3533 - accuracy: 0.8955 - val_loss: 0.2948 - val_accuracy: 0.9137\n",
            "\n",
            "Epoch 00009: val_accuracy improved from 0.90990 to 0.91370, saving model to weights\\weights_DNN_1.hdf5\n",
            "Epoch 10/10\n",
            "469/469 - 1s - loss: 0.3367 - accuracy: 0.9004 - val_loss: 0.2840 - val_accuracy: 0.9174\n",
            "\n",
            "Epoch 00010: val_accuracy improved from 0.91370 to 0.91740, saving model to weights\\weights_DNN_1.hdf5\n",
            "RNN  0 \n",
            "\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f3086290748>\n",
            "Epoch 1/10\n",
            "469/469 - 66s - loss: 0.7951 - accuracy: 0.7298 - val_loss: 0.3965 - val_accuracy: 0.8698\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.86980, saving model to weights\\weights_RNN_0.hdf5\n",
            "Epoch 2/10\n",
            "469/469 - 61s - loss: 0.2628 - accuracy: 0.9174 - val_loss: 0.1639 - val_accuracy: 0.9511\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.86980 to 0.95110, saving model to weights\\weights_RNN_0.hdf5\n",
            "Epoch 3/10\n",
            "469/469 - 61s - loss: 0.1447 - accuracy: 0.9547 - val_loss: 0.1103 - val_accuracy: 0.9667\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.95110 to 0.96670, saving model to weights\\weights_RNN_0.hdf5\n",
            "Epoch 4/10\n",
            "469/469 - 61s - loss: 0.0985 - accuracy: 0.9696 - val_loss: 0.0744 - val_accuracy: 0.9764\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.96670 to 0.97640, saving model to weights\\weights_RNN_0.hdf5\n",
            "Epoch 5/10\n",
            "469/469 - 62s - loss: 0.0772 - accuracy: 0.9758 - val_loss: 0.0568 - val_accuracy: 0.9825\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.97640 to 0.98250, saving model to weights\\weights_RNN_0.hdf5\n",
            "Epoch 6/10\n",
            "469/469 - 62s - loss: 0.0621 - accuracy: 0.9804 - val_loss: 0.0546 - val_accuracy: 0.9834\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.98250 to 0.98340, saving model to weights\\weights_RNN_0.hdf5\n",
            "Epoch 7/10\n",
            "469/469 - 63s - loss: 0.0540 - accuracy: 0.9839 - val_loss: 0.0462 - val_accuracy: 0.9861\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.98340 to 0.98610, saving model to weights\\weights_RNN_0.hdf5\n",
            "Epoch 8/10\n",
            "469/469 - 63s - loss: 0.0474 - accuracy: 0.9853 - val_loss: 0.0470 - val_accuracy: 0.9851\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.98610\n",
            "Epoch 9/10\n",
            "469/469 - 62s - loss: 0.0420 - accuracy: 0.9868 - val_loss: 0.0408 - val_accuracy: 0.9876\n",
            "\n",
            "Epoch 00009: val_accuracy improved from 0.98610 to 0.98760, saving model to weights\\weights_RNN_0.hdf5\n",
            "Epoch 10/10\n",
            "469/469 - 64s - loss: 0.0375 - accuracy: 0.9878 - val_loss: 0.0374 - val_accuracy: 0.9890\n",
            "\n",
            "Epoch 00010: val_accuracy improved from 0.98760 to 0.98900, saving model to weights\\weights_RNN_0.hdf5\n",
            "RNN  1 \n",
            "\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f303ef706a0>\n",
            "Epoch 1/10\n",
            "469/469 - 68s - loss: 2.3023 - accuracy: 0.1138 - val_loss: 2.3018 - val_accuracy: 0.1135\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.11350, saving model to weights\\weights_RNN_1.hdf5\n",
            "Epoch 2/10\n",
            "469/469 - 63s - loss: 2.3018 - accuracy: 0.1124 - val_loss: 2.3014 - val_accuracy: 0.1135\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.11350\n",
            "Epoch 3/10\n",
            "469/469 - 64s - loss: 2.3015 - accuracy: 0.1124 - val_loss: 2.3012 - val_accuracy: 0.1135\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.11350\n",
            "Epoch 4/10\n",
            "469/469 - 63s - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.11350\n",
            "Epoch 5/10\n",
            "469/469 - 63s - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3009 - val_accuracy: 0.1135\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.11350\n",
            "Epoch 6/10\n",
            "469/469 - 63s - loss: 2.3011 - accuracy: 0.1124 - val_loss: 2.3008 - val_accuracy: 0.1135\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.11350\n",
            "Epoch 7/10\n",
            "469/469 - 63s - loss: 2.3010 - accuracy: 0.1124 - val_loss: 2.3007 - val_accuracy: 0.1135\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.11350\n",
            "Epoch 8/10\n",
            "469/469 - 64s - loss: 2.3008 - accuracy: 0.1124 - val_loss: 2.3006 - val_accuracy: 0.1135\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.11350\n",
            "Epoch 9/10\n",
            "469/469 - 63s - loss: 2.3007 - accuracy: 0.1124 - val_loss: 2.3004 - val_accuracy: 0.1135\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.11350\n",
            "Epoch 10/10\n",
            "469/469 - 64s - loss: 2.3006 - accuracy: 0.1124 - val_loss: 2.3003 - val_accuracy: 0.1135\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.11350\n",
            "CNN  0 \n",
            "\n",
            "Error in model 0  try to re-generate another model\n",
            "CNN  0 \n",
            "\n",
            "Error in model 0  try to re-generate another model\n",
            "CNN  0 \n",
            "\n",
            "<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f303ef670f0>\n",
            "Epoch 1/10\n",
            "469/469 - 42s - loss: 0.4051 - accuracy: 0.9131 - val_loss: 0.0283 - val_accuracy: 0.9907\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.99070, saving model to weights\\weights_CNN_0.hdf5\n",
            "Epoch 2/10\n",
            "469/469 - 37s - loss: 0.0410 - accuracy: 0.9877 - val_loss: 0.0389 - val_accuracy: 0.9888\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.99070\n",
            "Epoch 3/10\n",
            "469/469 - 37s - loss: 0.0295 - accuracy: 0.9913 - val_loss: 0.0341 - val_accuracy: 0.9914\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.99070 to 0.99140, saving model to weights\\weights_CNN_0.hdf5\n",
            "Epoch 4/10\n",
            "469/469 - 37s - loss: 0.0233 - accuracy: 0.9935 - val_loss: 0.0295 - val_accuracy: 0.9916\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.99140 to 0.99160, saving model to weights\\weights_CNN_0.hdf5\n",
            "Epoch 5/10\n",
            "469/469 - 37s - loss: 0.0194 - accuracy: 0.9945 - val_loss: 0.0290 - val_accuracy: 0.9943\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.99160 to 0.99430, saving model to weights\\weights_CNN_0.hdf5\n",
            "Epoch 6/10\n",
            "469/469 - 37s - loss: 0.0169 - accuracy: 0.9951 - val_loss: 0.0370 - val_accuracy: 0.9902\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.99430\n",
            "Epoch 7/10\n",
            "469/469 - 37s - loss: 0.0163 - accuracy: 0.9956 - val_loss: 0.0233 - val_accuracy: 0.9934\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.99430\n",
            "Epoch 8/10\n",
            "469/469 - 37s - loss: 0.0137 - accuracy: 0.9967 - val_loss: 0.0285 - val_accuracy: 0.9942\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.99430\n",
            "Epoch 9/10\n",
            "469/469 - 37s - loss: 0.0136 - accuracy: 0.9966 - val_loss: 0.0289 - val_accuracy: 0.9937\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.99430\n",
            "Epoch 10/10\n",
            "469/469 - 37s - loss: 0.0114 - accuracy: 0.9971 - val_loss: 0.0418 - val_accuracy: 0.9942\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.99430\n",
            "CNN  1 \n",
            "\n",
            "Error in model 1  try to re-generate another model\n",
            "CNN  1 \n",
            "\n",
            "Error in model 1  try to re-generate another model\n",
            "CNN  1 \n",
            "\n",
            "Error in model 1  try to re-generate another model\n",
            "CNN  1 \n",
            "\n",
            "<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f303e2706d8>\n",
            "Epoch 1/10\n",
            "469/469 - 30s - loss: 2.2967 - accuracy: 0.1891 - val_loss: 2.2883 - val_accuracy: 0.3380\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.33800, saving model to weights\\weights_CNN_1.hdf5\n",
            "Epoch 2/10\n",
            "469/469 - 28s - loss: 2.2788 - accuracy: 0.3694 - val_loss: 2.2514 - val_accuracy: 0.5709\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.33800 to 0.57090, saving model to weights\\weights_CNN_1.hdf5\n",
            "Epoch 3/10\n",
            "469/469 - 28s - loss: 1.9860 - accuracy: 0.5331 - val_loss: 0.8354 - val_accuracy: 0.7872\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.57090 to 0.78720, saving model to weights\\weights_CNN_1.hdf5\n",
            "Epoch 4/10\n",
            "469/469 - 28s - loss: 0.5727 - accuracy: 0.8185 - val_loss: 0.2729 - val_accuracy: 0.9176\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.78720 to 0.91760, saving model to weights\\weights_CNN_1.hdf5\n",
            "Epoch 5/10\n",
            "469/469 - 28s - loss: 0.2947 - accuracy: 0.9092 - val_loss: 0.1806 - val_accuracy: 0.9412\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.91760 to 0.94120, saving model to weights\\weights_CNN_1.hdf5\n",
            "Epoch 6/10\n",
            "469/469 - 28s - loss: 0.2158 - accuracy: 0.9328 - val_loss: 0.1336 - val_accuracy: 0.9562\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.94120 to 0.95620, saving model to weights\\weights_CNN_1.hdf5\n",
            "Epoch 7/10\n",
            "469/469 - 28s - loss: 0.1739 - accuracy: 0.9461 - val_loss: 0.1114 - val_accuracy: 0.9645\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.95620 to 0.96450, saving model to weights\\weights_CNN_1.hdf5\n",
            "Epoch 8/10\n",
            "469/469 - 28s - loss: 0.1479 - accuracy: 0.9550 - val_loss: 0.0898 - val_accuracy: 0.9711\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.96450 to 0.97110, saving model to weights\\weights_CNN_1.hdf5\n",
            "Epoch 9/10\n",
            "469/469 - 28s - loss: 0.1317 - accuracy: 0.9592 - val_loss: 0.0833 - val_accuracy: 0.9730\n",
            "\n",
            "Epoch 00009: val_accuracy improved from 0.97110 to 0.97300, saving model to weights\\weights_CNN_1.hdf5\n",
            "Epoch 10/10\n",
            "469/469 - 28s - loss: 0.1184 - accuracy: 0.9640 - val_loss: 0.0807 - val_accuracy: 0.9735\n",
            "\n",
            "Epoch 00010: val_accuracy improved from 0.97300 to 0.97350, saving model to weights\\weights_CNN_1.hdf5\n",
            "(10000, 6)\n",
            "(10000, 6)\n",
            "Accuracy of 6 models: [0.9041, 0.9174, 0.989, 0.1135, 0.9943, 0.9735]\n",
            "Accuracy: 0.9943\n",
            "Accuracy F Score: 0.976\n",
            "F1_Micro: 0.976\n",
            "F1_Macro: 0.9759135661419315\n",
            "F1_weighted: 0.975965974832918\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}